{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import binomtest\n",
    "import json\n",
    "import operator\n",
    "\n",
    "import metrics\n",
    "\n",
    "round_precision = 4\n",
    "p_baseline = 0.5 # baseline for binomial test\n",
    "results_path = \"../results/\"\n",
    "time_results_column = \"mae\"\n",
    "\n",
    "\"\"\"\n",
    "more parameters \"condition\" fewer parameters \n",
    "\">\": operator.gt,\n",
    "\"<\": operator.lt,\n",
    "\"==\": operator.eq,\n",
    "\"!=\": operator.ne,\n",
    "\">=\": operator.ge,\n",
    "\"<=\": operator.le,\n",
    "\"\"\"\n",
    "\n",
    "bionm_tests = [\n",
    "    {\n",
    "        \"name\": \"Range\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.range,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Standard Deviation\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.standard_deviation,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Skewness\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.absolute_value_skew,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Lower Fence Outliers\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.lower_fence_outliers,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Upper Fence Outliers\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.upper_fence_outliers,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "        {\n",
    "        \"name\": \"All Outliers\",\n",
    "        \"sort_criteria\": metrics.parameters,\n",
    "        \"metric\": metrics.all_outliers,\n",
    "        \"condition\": operator.lt,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(file, return_error):\n",
    "\n",
    "    if \"image_classification\" in file:\n",
    "        df = pd.read_csv(file)\n",
    "        if return_error:\n",
    "            return metrics.accuracy_to_error(df[\"test_accuracy\"].tolist())\n",
    "        else:\n",
    "            return df[\"test_accuracy\"].tolist()\n",
    "    elif \"time_series\" in file:\n",
    "        df = pd.read_csv(file)\n",
    "        return df[time_results_column].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_k_same_total(test_results, test_metric, condition):\n",
    "    total = 0\n",
    "    k = 0\n",
    "    same = 0\n",
    "\n",
    "    # Compare the results one by one\n",
    "    for i in range(len(test_results) - 1):\n",
    "        for j in range(i + 1, len(test_results)):\n",
    "            \n",
    "            greater_distribution = test_results[i][\"distribution\"]\n",
    "            greater_results = test_metric(greater_distribution)\n",
    "\n",
    "            less_distribution = test_results[j][\"distribution\"]\n",
    "            less_results = test_metric(less_distribution)\n",
    "\n",
    "            # Compare the results values\n",
    "            if condition(greater_results, less_results):\n",
    "                k += 1\n",
    "            elif greater_results == less_results:\n",
    "                same += 1\n",
    "            \n",
    "            total += 1\n",
    "\n",
    "    return k, same, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification Intra-architecture Binomial Comparison\n",
    "image_classification_model_tests = json.load(open(\"image_classification_intra_architecture.json\", \"r\"))\n",
    "\n",
    "print(\"Test & # Larger Models & # Smaller Models & # Tied Models & # Total Models & Baseline p-value & Larger p-value & Smaller p-value \\\\\\\\\")\n",
    "\n",
    "for bionm_test in bionm_tests:\n",
    "\n",
    "    total = 0\n",
    "    k = 0\n",
    "    same = 0\n",
    "\n",
    "    for model_test in image_classification_model_tests:\n",
    "        \n",
    "        image_classification_test_results = []\n",
    "\n",
    "        for model in model_test[\"models\"]:\n",
    "            # Get the file name\n",
    "            file = f\"{model}-{model_test['dataset']}-idun-A100-PyTorch-ngc2312\"\n",
    "            if model_test[\"pretrained\"]:\n",
    "                file += \"-pretrained\"\n",
    "            file += \".csv\"\n",
    "\n",
    "            # Get the distribution\n",
    "            distribution = get_distribution(results_path + \"image_classification/\" + file, return_error=False)\n",
    "\n",
    "            # Get the sort_value from the sort_criteria\n",
    "            if bionm_test[\"sort_criteria\"] == metrics.parameters:\n",
    "                sort_value = metrics.parameters(model)\n",
    "            else:\n",
    "                sort_value = bionm_test[\"sort_criteria\"](distribution)\n",
    "\n",
    "            image_classification_test_results.append({\n",
    "                \"test\": file,\n",
    "                \"distribution\": distribution,\n",
    "                \"sort\": float(sort_value),\n",
    "            })\n",
    "\n",
    "        # Sort the test results by the sort key descending order (largest sort value first)\n",
    "        image_classification_test_results = sorted(image_classification_test_results, key=lambda x: x[\"sort\"], reverse=True)\n",
    "\n",
    "        x, y, z = calculate_k_same_total(image_classification_test_results, bionm_test[\"metric\"], bionm_test[\"condition\"])\n",
    "\n",
    "        k += x\n",
    "        same += y\n",
    "        total += z\n",
    "\n",
    "    greater = k\n",
    "    less = total - k - same\n",
    "\n",
    "    # Discard the trial: You can remove the tied trial from your analysis and only include clear successes or failures.\n",
    "    total = total - same\n",
    "\n",
    "    binom_results = binomtest(k=greater, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    geater_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    binom_results = binomtest(k=less, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    less_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    print(f\"{bionm_test['name']} & {greater} & {less} & {same} & {total} & {p_baseline} & {geater_p_value} & {less_p_value} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image Classification Cross-architecture Binomial Comparison\n",
    "image_classification_model_tests = json.load(open(\"image_classification_cross_architecture.json\", \"r\"))\n",
    "\n",
    "print(\"Test & # Larger Models & # Smaller Models & # Tied Models & # Total Models & Baseline p-value & Larger p-value & Smaller p-value \\\\\\\\\")\n",
    "\n",
    "for bionm_test in bionm_tests:\n",
    "\n",
    "    total = 0\n",
    "    k = 0\n",
    "    same = 0\n",
    "\n",
    "    for model_test in image_classification_model_tests:\n",
    "        \n",
    "        image_classification_test_results = []\n",
    "\n",
    "        for model in model_test[\"models\"]:\n",
    "            # Get the file name\n",
    "            file = f\"{model}-{model_test['dataset']}-idun-A100-PyTorch-ngc2312\"\n",
    "            if model_test[\"pretrained\"]:\n",
    "                file += \"-pretrained\"\n",
    "            file += \".csv\"\n",
    "\n",
    "            # Get the distribution\n",
    "            distribution = get_distribution(results_path + \"image_classification/\" + file, return_error=False)\n",
    "\n",
    "            # Get the sort_value from the sort_criteria\n",
    "            if bionm_test[\"sort_criteria\"] == metrics.parameters:\n",
    "                sort_value = metrics.parameters(model)\n",
    "            else:\n",
    "                sort_value = bionm_test[\"sort_criteria\"](distribution)\n",
    "\n",
    "            image_classification_test_results.append({\n",
    "                \"test\": file,\n",
    "                \"distribution\": distribution,\n",
    "                \"sort\": float(sort_value),\n",
    "            })\n",
    "\n",
    "        # Sort the test results by the sort key descending order (largest sort value first)\n",
    "        image_classification_test_results = sorted(image_classification_test_results, key=lambda x: x[\"sort\"], reverse=True)\n",
    "\n",
    "        x, y, z = calculate_k_same_total(image_classification_test_results, bionm_test[\"metric\"], bionm_test[\"condition\"])\n",
    "\n",
    "        k += x\n",
    "        same += y\n",
    "        total += z\n",
    "\n",
    "    greater = k\n",
    "    less = total - k - same\n",
    "\n",
    "    # Discard the trial: You can remove the tied trial from your analysis and only include clear successes or failures.\n",
    "    total = total - same\n",
    "\n",
    "    binom_results = binomtest(k=greater, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    geater_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    binom_results = binomtest(k=less, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    less_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    print(f\"{bionm_test['name']} & {greater} & {less} & {same} & {total} & {p_baseline} & {geater_p_value} & {less_p_value} \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Cross-architecture Binomial Comparison\n",
    "time_series_model_tests = json.load(open(\"time_series_model_tests.json\", \"r\"))\n",
    "\n",
    "print(\"Test & # Larger Models & # Smaller Models & # Tied Models & # Total Models & Baseline p-value & Larger p-value & Smaller p-value \\\\\\\\\")\n",
    "\n",
    "for bionm_test in bionm_tests:\n",
    "\n",
    "    total = 0\n",
    "    k = 0\n",
    "    same = 0\n",
    "\n",
    "    for model_test in time_series_model_tests:\n",
    "        \n",
    "        time_series_test_results = []\n",
    "\n",
    "        for model in model_test[\"models\"]:\n",
    "            # Get the file name\n",
    "            file = f\"{model}_{model_test['dataset']}_96_{model_test['horizon']}_0_100.csv\"\n",
    "\n",
    "            # Get the distribution\n",
    "            distribution = get_distribution(results_path + \"time_series/\" + file, return_error=True)\n",
    "\n",
    "            # Get the sort_value from the sort_criteria\n",
    "            if bionm_test[\"sort_criteria\"] == metrics.parameters:\n",
    "                sort_value = metrics.parameters(model, model_test['dataset'], model_test['horizon'])\n",
    "            else:\n",
    "                sort_value = bionm_test[\"sort_criteria\"](distribution)\n",
    "\n",
    "            time_series_test_results.append({\n",
    "                \"test\": file,\n",
    "                \"distribution\": distribution,\n",
    "                \"horizon\": model_test[\"horizon\"],\n",
    "                \"sort\": float(sort_value),\n",
    "            })\n",
    "\n",
    "        # Sort the test results by the sort key descending order (largest sort value first)\n",
    "        time_series_test_results = sorted(time_series_test_results, key=lambda x: x[\"sort\"], reverse=True)\n",
    "\n",
    "        x, y, z = calculate_k_same_total(time_series_test_results, bionm_test[\"metric\"], bionm_test[\"condition\"])\n",
    "\n",
    "        k += x\n",
    "        same += y\n",
    "        total += z\n",
    "\n",
    "    greater = k\n",
    "    less = total - k - same\n",
    "\n",
    "    # Discard the trial: You can remove the tied trial from your analysis and only include clear successes or failures.\n",
    "    total = total - same\n",
    "\n",
    "    binom_results = binomtest(k=greater, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    geater_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    binom_results = binomtest(k=less, n=total, p=p_baseline, alternative='greater')\n",
    "\n",
    "    less_p_value = metrics.safe_round(binom_results.pvalue, round_precision)\n",
    "\n",
    "    print(f\"{bionm_test['name']} & {greater} & {less} & {same} & {total} & {p_baseline} & {geater_p_value} & {less_p_value} \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
